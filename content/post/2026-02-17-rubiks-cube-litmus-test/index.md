---
title: 魔方试纸
date: 2026-02-17
categories:
tags:
  - thinking
  - tech
  - llm
  - ai
  - rubiks-cube
  - reasoning
slug: rubiks-cube-litmus-test
originalLang: zh
---

最近在跟小朋友一起学三阶魔方。

我之前就会基础的层先法，能还原但速度很慢。这次想趁着跟孩子一起玩的机会，把第三层的解法从层先法升级到 CFOP——也就是学 OLL 和 PLL 那套更系统的公式。斯坦福有一份两三页的[教程](https://cube.stanford.edu/class/files/rubiks_cube_solution.pdf)，后半部分讲的就是这些内容，每种状态对应什么公式、怎么识别朝向，写得很清楚。照着学，并不难。

但我想偷个懒，让大模型帮我把 OLL 和 PLL 的教学步骤重新梳理一下，省得自己对着 PDF 一点点翻。

结果发现——它做不到。

---

不是说它完全不懂。每个模型都能聊 CFOP 的基本框架，能提到 OLL、PLL 各有多少种情况，甚至能给出看起来有模有样的公式序列。问题在于，你真拿着魔方照它说的拧，拧不对。

这里公式的前提条件漏了，那里默认的朝向跟你手上的状态对不上。你指出来，它道歉，换一种说法，但新的说法又在别的环节出了偏差。来来回回好几轮，始终无法收敛到一条完全正确的路径。

我试了几个主流模型，无一例外。

最后还是回到那份斯坦福的 PDF，两三页纸，老老实实对照着学。人写的教程，一次就对了。

---

这件事让我觉得很拧巴。

因为我平时确实非常依赖大模型，而且它在很多场景下表现得极其出色。写代码的时候，它是真正的效率倍增器——查 API、debug、理顺混乱的逻辑，都是它的强项。翻译、信息整理、辅助思考，这些事情它做得又快又好。有些时候我问它一些比较深的问题，它给出的回答结构清晰、有洞察力，比大多数人类对话有营养得多。

那种时刻，你会真心觉得这个东西很厉害。

但就是这个很厉害的东西，面对 OLL 和 PLL 的教学梳理——一个照着公式表就能搞定的任务——它就是做不对。不是完全不会，是永远“差一点”。

---

后来我想，这不是“偶尔犯错”的问题，它暴露的是大模型一个更根本的特性。

三阶魔方本质上是一个严格的状态机。54 个色块，每转一下整体状态就变了。要解对，必须在每一步都精确知道当前状态是什么、公式的前提是否满足、执行之后状态会变成什么。这是一个需要精确跟踪和逐步验证的问题。

而大模型并没有在内部真的“转”一个魔方。它没有维护一个魔方对象，没有在做群运算。它做的是根据训练数据中见过的大量魔方教程，生成概率上“最像正确答案”的文本。它“知道”某种 OLL 状态大概率对应某个公式，但不会验证当前朝向是否真的满足前提，也不会在输出公式后检查结果是否正确。你告诉它上一步错了，它也不是真的回溯推理链，只是继续往前生成新的文本。

它是在“说”解法，不是在“做”解法。

魔方之所以把这个问题暴露得如此彻底，是因为它有一个残酷的特性：零容错，即时验证。对就是对，错就是错，六面要么齐了要么没齐，没有“差不多”的余地。在这种环境下，大模型基于统计生成的本质就彻底藏不住了。

---

这件事让我重新思考了一个一直模糊的问题：我们到底该怎么理解大模型的能力边界？

它很强大，这一点毫无疑问。写代码、翻译、辅助思考、整理信息——这些场景下它的价值实实在在，我自己每天都在受益。但它有一个不容易被察觉的问题：它的任务边界不清晰。

一个传统的软件工具，能做什么、不能做什么，边界是明确的。大模型不一样。它什么都能接，什么问题都敢答，而且大部分时候答得还不错。这就很容易让人放松警惕——它既然能帮我写出复杂的代码，那梳理几个 OLL 公式总不在话下吧？

然而恰恰是在这种“理应不会出错”的地方，它翻了车。

这不是个案。大模型本质上是一个概率系统，输出带有内在的随机性。这意味着使用者必须始终对它的输出保持判断力——在你熟悉的领域，它犯的错你能发现；但在你不熟悉的领域，你甚至不知道它错了。

所以我现在的认识是：大模型目前最合适的定位，仍然是一个人类助手。一个能力极强的助手，但终究是助手。不能把任务扔给它就不管了，必须在实际使用中审慎地界定它的任务边界——哪些环节可以放心交给它，哪些环节必须人工把关。这条边界如果划不清楚，那些低级但致命的错误，就会在最意想不到的地方冒出来。

---

还有一层更深的思考。

魔方这件事让我隐约意识到，大模型当前的技术范式也许存在某种结构性的局限。它在语言理解和生成上已经强大到令人惊叹，但面对需要精确状态维护和严格逻辑校验的任务，它的架构似乎天然不适配。这恐怕不是“再多训练一些数据”或者“把模型再做大一些”能够弥补的——也许需要某种更根本的范式演进，才能跨过这道坎。

具体会是什么样的演进，我不知道。但我觉得，一个两三页纸就能教会人类的问题，却能让当前最先进的大模型反复出错——这个事实本身就值得认真对待。

一个三阶魔方，竟然成了检验大模型能力边界最直观的试纸。
